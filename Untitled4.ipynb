{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip uninstall tabula tabula-py\n",
        "!pip install tabula-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnAujK0BTYma",
        "outputId": "57dab35b-253d-4559-d649-9d7332f9dd20"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tabula as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tabula-py 2.10.0\n",
            "Uninstalling tabula-py-2.10.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.11/dist-packages/tabula/*\n",
            "    /usr/local/lib/python3.11/dist-packages/tabula_py-2.10.0.dist-info/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled tabula-py-2.10.0\n",
            "Collecting tabula-py\n",
            "  Using cached tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.11/dist-packages (from tabula-py) (2.3.0)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.11/dist-packages (from tabula-py) (1.26.4)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.11/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Using cached tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "Installing collected packages: tabula-py\n",
            "Successfully installed tabula-py-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8T30zpeLxeOm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d93a83e-5549-4ffe-d6a0-7daa2a0682f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: boto3>=1.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (1.39.0)\n",
            "Requirement already satisfied: botocore>=1.34.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (1.39.0)\n",
            "Requirement already satisfied: langchain>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (0.3.26)\n",
            "Requirement already satisfied: langchain-aws>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.2.27)\n",
            "Requirement already satisfied: langchain-community>=0.2.17 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (0.3.26)\n",
            "Requirement already satisfied: PyMuPDF>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 9)) (1.26.1)\n",
            "Requirement already satisfied: tabula-py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 10)) (2.10.0)\n",
            "Requirement already satisfied: faiss-cpu>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 13)) (1.11.0)\n",
            "Requirement already satisfied: tqdm>=4.66.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 16)) (4.67.1)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 17)) (2.32.3)\n",
            "Requirement already satisfied: python-dotenv>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 18)) (1.1.1)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.34.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.34.0->-r requirements.txt (line 2)) (0.13.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.34.0->-r requirements.txt (line 3)) (2.9.0.post0)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore>=1.34.0->-r requirements.txt (line 3)) (2.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (0.3.66)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (0.3.8)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (2.0.41)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain>=0.1.0->-r requirements.txt (line 4)) (6.0.2)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.11/dist-packages (from langchain-aws>=0.1.0->-r requirements.txt (line 5)) (1.26.4)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2.17->-r requirements.txt (line 6)) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2.17->-r requirements.txt (line 6)) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2.17->-r requirements.txt (line 6)) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2.17->-r requirements.txt (line 6)) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community>=0.2.17->-r requirements.txt (line 6)) (0.4.1)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.11/dist-packages (from tabula-py>=2.9.0->-r requirements.txt (line 10)) (2.3.0)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.11/dist-packages (from tabula-py>=2.9.0->-r requirements.txt (line 10)) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu>=1.8.0->-r requirements.txt (line 13)) (24.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 17)) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 17)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31.0->-r requirements.txt (line 17)) (2025.6.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community>=0.2.17->-r requirements.txt (line 6)) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.17->-r requirements.txt (line 6)) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.17->-r requirements.txt (line 6)) (0.9.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.1.0->-r requirements.txt (line 4)) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain>=0.1.0->-r requirements.txt (line 4)) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (0.23.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py>=2.9.0->-r requirements.txt (line 10)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.25.3->tabula-py>=2.9.0->-r requirements.txt (line 10)) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 4)) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain>=0.1.0->-r requirements.txt (line 4)) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.34.0->-r requirements.txt (line 3)) (1.17.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.1.0->-r requirements.txt (line 4)) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain>=0.1.0->-r requirements.txt (line 4)) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community>=0.2.17->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain>=0.1.0->-r requirements.txt (line 4)) (1.3.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "import tabula\n",
        "import faiss\n",
        "import json\n",
        "import base64\n",
        "import pymupdf\n",
        "import requests\n",
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from botocore.exceptions import ClientError\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from IPython import display\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.setLevel(logging.ERROR)"
      ],
      "metadata": {
        "id": "_Eturs_H0ZbP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the dataset - URL of the \"Attention Is All You Need\" paper (Replace it with the URL of the PDF file/dataset you want to download)\n",
        "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
        "\n",
        "# Set the filename and filepath\n",
        "filename = \"attention_paper.pdf\"\n",
        "filepath = os.path.join(\"data\", filename)\n",
        "\n",
        "# Create the data directory if it doesn't exist\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "\n",
        "# Download the file\n",
        "response = requests.get(url)\n",
        "if response.status_code == 200:\n",
        "    with open(filepath, 'wb') as file:\n",
        "        file.write(response.content)\n",
        "    print(f\"File downloaded successfully: {filepath}\")\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
      ],
      "metadata": {
        "id": "QLeO3Mlf10Dw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "128e3cf9-5d65-441f-a510-b20c9498e4c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File downloaded successfully: data/attention_paper.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the directories\n",
        "def create_directories(base_dir):\n",
        "    directories = [\"images\", \"text\", \"tables\", \"page_images\"]\n",
        "    for dir in directories:\n",
        "        os.makedirs(os.path.join(base_dir, dir), exist_ok=True)\n",
        "\n",
        "# Process tables\n",
        "def process_tables(doc, page_num, base_dir, items):\n",
        "    try:\n",
        "        tables = tabula.read_pdf(filepath, pages=page_num + 1, multiple_tables=True)\n",
        "        if not tables:\n",
        "            return\n",
        "        for table_idx, table in enumerate(tables):\n",
        "            table_text = \"\\n\".join([\" | \".join(map(str, row)) for row in table.values])\n",
        "            table_file_name = f\"{base_dir}/tables/{os.path.basename(filepath)}_table_{page_num}_{table_idx}.txt\"\n",
        "            with open(table_file_name, 'w') as f:\n",
        "                f.write(table_text)\n",
        "            items.append({\"page\": page_num, \"type\": \"table\", \"text\": table_text, \"path\": table_file_name})\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting tables from page {page_num}: {str(e)}\")\n",
        "\n",
        "# Process text chunks\n",
        "def process_text_chunks(text, text_splitter, page_num, base_dir, items):\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        text_file_name = f\"{base_dir}/text/{os.path.basename(filepath)}_text_{page_num}_{i}.txt\"\n",
        "        with open(text_file_name, 'w') as f:\n",
        "            f.write(chunk)\n",
        "        items.append({\"page\": page_num, \"type\": \"text\", \"text\": chunk, \"path\": text_file_name})\n",
        "\n",
        "# Process images\n",
        "def process_images(page, page_num, base_dir, items):\n",
        "    images = page.get_images()\n",
        "    for idx, image in enumerate(images):\n",
        "        xref = image[0]\n",
        "        pix = pymupdf.Pixmap(doc, xref)\n",
        "        image_name = f\"{base_dir}/images/{os.path.basename(filepath)}_image_{page_num}_{idx}_{xref}.png\"\n",
        "        pix.save(image_name)\n",
        "        with open(image_name, 'rb') as f:\n",
        "            encoded_image = base64.b64encode(f.read()).decode('utf8')\n",
        "        items.append({\"page\": page_num, \"type\": \"image\", \"path\": image_name, \"image\": encoded_image})\n",
        "\n",
        "# Process page images\n",
        "def process_page_images(page, page_num, base_dir, items):\n",
        "    pix = page.get_pixmap()\n",
        "    page_path = os.path.join(base_dir, f\"page_images/page_{page_num:03d}.png\")\n",
        "    pix.save(page_path)\n",
        "    with open(page_path, 'rb') as f:\n",
        "        page_image = base64.b64encode(f.read()).decode('utf8')\n",
        "    items.append({\"page\": page_num, \"type\": \"page\", \"path\": page_path, \"image\": page_image})"
      ],
      "metadata": {
        "id": "YaEfCRlR15r4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = pymupdf.open(filepath)\n",
        "num_pages = len(doc)\n",
        "base_dir = \"data\"\n",
        "\n",
        "# Creating the directories\n",
        "create_directories(base_dir)\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=200, length_function=len)\n",
        "items = []\n",
        "\n",
        "# Process each page of the PDF\n",
        "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
        "    page = doc[page_num]\n",
        "    text = page.get_text()\n",
        "    process_tables(doc, page_num, base_dir, items)\n",
        "    process_text_chunks(text, text_splitter, page_num, base_dir, items)\n",
        "    process_images(page, page_num, base_dir, items)\n",
        "    process_page_images(page, page_num, base_dir, items)"
      ],
      "metadata": {
        "id": "0UKzuT2d2Ehk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b821fd19-adaf-4a88-b6aa-3e3123fc9b0a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing PDF pages:   0%|          | 0/15 [00:00<?, ?it/s]WARNING:tabula.backend:Failed to import jpype dependencies. Fallback to subprocess.\n",
            "WARNING:tabula.backend:No module named 'jpype'\n",
            "WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:45:52 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider loadDiskCache\n",
            "WARNING: New fonts found, font cache will be re-built\n",
            "Jul 01, 2025 6:45:52 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Building on-disk font cache, this may take a while\n",
            "Jul 01, 2025 6:45:53 PM org.apache.pdfbox.pdmodel.font.FileSystemFontProvider <init>\n",
            "WARNING: Finished building on-disk font cache, found 17 fonts\n",
            "Jul 01, 2025 6:45:53 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:45:53 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:   7%|▋         | 1/15 [00:04<00:57,  4.11s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:45:58 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:45:58 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  13%|█▎        | 2/15 [00:12<01:23,  6.43s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:06 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:06 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  20%|██        | 3/15 [00:24<01:47,  8.92s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:18 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:18 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "Jul 01, 2025 6:46:18 PM org.apache.pdfbox.pdmodel.font.PDSimpleFont toUnicode\n",
            "WARNING: No Unicode mapping for summationtext (80) in font THPNLT+CMEX9\n",
            "\n",
            "Processing PDF pages:  27%|██▋       | 4/15 [00:31<01:29,  8.16s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:24 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:24 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  33%|███▎      | 5/15 [00:35<01:08,  6.90s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:28 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:28 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  40%|████      | 6/15 [00:39<00:52,  5.79s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:31 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:31 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  47%|████▋     | 7/15 [00:42<00:40,  5.01s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:35 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:35 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  53%|█████▎    | 8/15 [00:47<00:34,  4.98s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:40 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:40 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  60%|██████    | 9/15 [00:51<00:27,  4.52s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:43 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:43 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  67%|██████▋   | 10/15 [00:55<00:21,  4.39s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:47 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:47 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  73%|███████▎  | 11/15 [00:59<00:17,  4.41s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:52 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:52 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  80%|████████  | 12/15 [01:03<00:12,  4.33s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:46:56 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:46:56 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  87%|████████▋ | 13/15 [01:08<00:09,  4.54s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:47:02 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:47:02 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages:  93%|█████████▎| 14/15 [01:17<00:05,  5.69s/it]WARNING:tabula.backend:Got stderr: Jul 01, 2025 6:47:10 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font Symbol\n",
            "Jul 01, 2025 6:47:10 PM org.apache.pdfbox.pdmodel.font.PDType1Font <init>\n",
            "WARNING: Using fallback font LiberationSans for base font ZapfDingbats\n",
            "\n",
            "Processing PDF pages: 100%|██████████| 15/15 [01:25<00:00,  5.69s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the first text item\n",
        "[i for i in items if i['type'] == 'text'][0]"
      ],
      "metadata": {
        "id": "WHk_5NY02pT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb2a3fbf-0c8a-49b1-9a8e-3a6cd17183b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page': 0,\n",
              " 'type': 'text',\n",
              " 'text': 'Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or',\n",
              " 'path': 'data/text/attention_paper.pdf_text_0_0.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the first table item\n",
        "[i for i in items if i['type'] == 'table'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s7OqRXG7N6Q",
        "outputId": "34c6c399-7e46-4ced-acf8-b51121c5b871"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'page': 5,\n",
              " 'type': 'table',\n",
              " 'text': 'nan | nan | Operations | nan\\nSelf-Attention | O(n2 · d) | O(1) | O(1)\\nRecurrent | O(n · d2) | O(n) | O(n)\\nConvolutional | O(k · n · d2) | O(1) | O(logk(n))\\nSelf-Attention (restricted) | O(r · n · d) | O(1) | O(n/r)',\n",
              " 'path': 'data/tables/attention_paper.pdf_table_5_0.txt'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Multimodal Embeddings using Amazon Titan Multimodal Embeddings model\n",
        "import getpass\n",
        "\n",
        "# Prompt for credentials securely using getpass\n",
        "aws_access_key_id = getpass.getpass(\"Enter your AWS Access Key ID: \")\n",
        "aws_secret_access_key = getpass.getpass(\"Enter your AWS Secret Access Key: \")\n",
        "aws_region = input(\"Enter your AWS Region (e.g., us-east-1): \")\n",
        "\n",
        "# Set the credentials as environment variables for the current session\n",
        "os.environ[\"AWS_ACCESS_KEY_ID\"] = aws_access_key_id\n",
        "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = aws_secret_access_key\n",
        "os.environ[\"AWS_REGION\"] = aws_region\n",
        "\n",
        "def generate_multimodal_embeddings(prompt=None, image=None, output_embedding_length=384):\n",
        "    \"\"\"\n",
        "    Invoke the Amazon Titan Multimodal Embeddings model using Amazon Bedrock runtime.\n",
        "\n",
        "    Args:\n",
        "        prompt (str): The text prompt to provide to the model.\n",
        "        image (str): A base64-encoded image data.\n",
        "    Returns:\n",
        "        str: The model's response embedding.\n",
        "    \"\"\"\n",
        "    if not prompt and not image:\n",
        "        raise ValueError(\"Please provide either a text prompt, base64 image, or both as input\")\n",
        "\n",
        "    # Initialize the Amazon Bedrock runtime client\n",
        "    client = boto3.client(service_name=\"bedrock-runtime\", region_name=\"ap-south-1\")\n",
        "    model_id = \"amazon.titan-embed-image-v1\"\n",
        "\n",
        "    body = {\"embeddingConfig\": {\"outputEmbeddingLength\": output_embedding_length}}\n",
        "\n",
        "    if prompt:\n",
        "        body[\"inputText\"] = prompt\n",
        "    if image:\n",
        "        body[\"inputImage\"] = image\n",
        "\n",
        "    try:\n",
        "        response = client.invoke_model(\n",
        "            modelId=model_id,\n",
        "            body=json.dumps(body),\n",
        "            accept=\"application/json\",\n",
        "            contentType=\"application/json\"\n",
        "        )\n",
        "\n",
        "        # Process and return the response\n",
        "        result = json.loads(response.get(\"body\").read())\n",
        "        return result.get(\"embedding\")\n",
        "\n",
        "    except ClientError as err:\n",
        "        print(f\"Couldn't invoke Titan embedding model. Error: {err.response['Error']['Message']}\")\n",
        "        return None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxjH-LI67Qw5",
        "outputId": "b86ceb84-5ef4-4f8a-a68c-8579abba5387"
      },
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your AWS Access Key ID: ··········\n",
            "Enter your AWS Secret Access Key: ··········\n",
            "Enter your AWS Region (e.g., us-east-1): ap-south-1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set embedding vector dimension\n",
        "embedding_vector_dimension = 384\n",
        "\n",
        "# Count the number of each type of item\n",
        "item_counts = {\n",
        "    'text': sum(1 for item in items if item['type'] == 'text'),\n",
        "    'table': sum(1 for item in items if item['type'] == 'table'),\n",
        "    'image': sum(1 for item in items if item['type'] == 'image'),\n",
        "    'page': sum(1 for item in items if item['type'] == 'page')\n",
        "}\n",
        "\n",
        "# Initialize counters\n",
        "counters = dict.fromkeys(item_counts.keys(), 0)\n",
        "\n",
        "# Generate embeddings for all items\n",
        "with tqdm(\n",
        "    total=len(items),\n",
        "    desc=\"Generating embeddings\",\n",
        "    bar_format=(\n",
        "        \"{l_bar}{bar}| {n_fmt}/{total_fmt} \"\n",
        "        \"[{elapsed}<{remaining}, {rate_fmt}{postfix}]\"\n",
        "    )\n",
        ") as pbar:\n",
        "\n",
        "    for item in items:\n",
        "        item_type = item['type']\n",
        "        counters[item_type] += 1\n",
        "\n",
        "        if item_type in ['text', 'table']:\n",
        "            # For text or table, use the formatted text representation\n",
        "            item['embedding'] = generate_multimodal_embeddings(prompt=item['text'],output_embedding_length=embedding_vector_dimension)\n",
        "        else:\n",
        "            # For images, use the base64-encoded image data\n",
        "            item['embedding'] = generate_multimodal_embeddings(image=item['image'], output_embedding_length=embedding_vector_dimension)\n",
        "\n",
        "        # Update the progress bar\n",
        "        pbar.set_postfix_str(f\"Text: {counters['text']}/{item_counts['text']}, Table: {counters['table']}/{item_counts['table']}, Image: {counters['image']}/{item_counts['image']}\")\n",
        "        pbar.update(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbPc5w9P7Zf9",
        "outputId": "6304d2fd-d123-45eb-d032-4d3a63867b99"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating embeddings: 100%|██████████| 108/108 [01:40<00:00,  1.08it/s, Text: 83/83, Table: 7/7, Image: 3/3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# All the embeddings\n",
        "all_embeddings = np.array([item['embedding'] for item in items])\n",
        "\n",
        "print(\"A single embedding:\", items[0]['embedding'])\n",
        "\n",
        "# Create FAISS Index\n",
        "index = faiss.IndexFlatL2(embedding_vector_dimension)\n",
        "\n",
        "# Clear any pre-existing index\n",
        "index.reset()\n",
        "\n",
        "# Add embeddings to the index\n",
        "index.add(np.array(all_embeddings, dtype=np.float32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfOzWAxFFCVm",
        "outputId": "83bb790a-b04c-4f03-edfb-aa0da1b47e35"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A single embedding: [0.003961269, 0.011774781, -0.012865039, -0.0016172152, 0.08373178, -0.032271624, 0.03169015, 0.029800372, 0.06715986, -0.019043164, -0.07617266, -0.020932944, 0.091872364, 0.0058873906, -0.06047295, 0.047680594, -0.06803207, -0.031399414, -0.025730077, 0.08140589, -0.055239715, 0.02238662, -0.019043164, -0.03808633, 0.010248421, -0.0014445912, -0.01271967, -0.040993683, -0.051460154, 0.0012810526, -0.00872206, 0.07908001, -0.012501619, 0.09536119, -0.016063128, 0.00795888, 0.017008018, -0.0242764, -0.013955296, -0.027183754, -0.021514414, -0.053204566, -0.018461693, -0.047099125, 0.0121382, 0.045354713, -0.020932944, 0.019188533, -0.08373178, 0.011265994, -0.13199385, 0.003470653, -0.0633803, -0.012428936, -0.051750887, 0.004906159, -0.0023803955, -0.09768707, -0.06425251, -0.02180515, -0.0034524822, 0.036487285, -0.018534377, -0.05058795, 0.080242954, -0.021950517, -0.04244736, -0.049715742, 0.0054512876, 0.03241699, -0.029509636, 0.08838354, -0.034452137, 0.0625081, 0.029945739, -0.04244736, -0.035760447, 0.07908001, -0.040702946, 0.04215662, -0.020642208, 0.020787576, 0.037214123, -0.086057656, -0.05291383, -0.05611192, 0.111060895, 0.0010630011, -0.03488824, -0.0242764, 0.06163589, 0.0027256438, -0.06628766, 0.018897796, -0.036487285, -0.08896501, -0.03983074, -0.020206105, -0.021950517, 0.017880224, -0.05872854, 0.07123016, -0.062217362, -0.022531988, -0.014972869, -0.025148606, -0.005778365, -0.05058795, 0.09826854, -0.0039249267, -0.0014718476, -0.03183552, 0.02674765, -0.05611192, -0.02936427, -0.045645446, 0.075591184, -0.063961774, 0.023840297, -0.075591184, -0.06861354, -0.019333899, -0.0037432173, 0.04622692, 0.019624636, 0.07123016, 0.0073410673, -0.043319564, 0.118620016, -0.052041624, -0.030963313, 0.052041624, 0.07675413, -0.056984127, 0.022968091, -0.008213273, 0.118620016, -0.044482507, 0.059891477, -0.03052721, -0.038958535, -0.05029721, 0.020351473, 0.008213273, 0.063961774, 0.030091107, 0.009521582, -0.005233236, -0.013591877, 0.15467119, -0.00854035, 0.026602283, -0.018389009, -0.02863743, 0.054076772, -0.01780754, -0.01780754, -0.04477324, -0.023985665, 0.00854035, -0.1290865, 0.05087868, 0.025875444, -0.020932944, -0.026020812, 0.04419177, 0.015554341, -0.06628766, 0.075591184, 0.013446509, -0.029073533, -0.10001295, -0.0058873906, -0.038667798, 0.040993683, -0.041284416, 0.06599692, -0.016426546, 0.021514414, 0.052623093, 0.029073533, -0.0018625233, -0.11396825, -0.07093942, -0.06047295, 0.030091107, -0.051750887, 0.021659782, -0.083150305, 0.09419825, 0.102338836, -0.03314383, -0.0683228, -0.0025257631, 0.0037250465, 0.038377065, 0.07093942, 0.041575152, 0.0625081, 0.029655004, -0.053786036, 0.009158163, 0.018607061, -0.05058795, -0.010248421, 0.015045553, -0.0062871515, 0.04913427, 0.0010630011, -0.04477324, 0.08780207, 0.04797133, 0.051750887, 0.049715742, -0.030381842, -0.032271624, -0.0073410673, -0.011120627, 0.00066778273, 0.03619655, 0.057856333, -0.077335596, 0.010902575, 0.007704486, 0.2023518, 0.0170807, 0.031544782, 0.035178974, -0.011484046, 0.051169418, 0.056984127, -0.027038386, -0.05233236, -0.014391399, 0.0930353, -0.0625081, -0.022095885, 0.0032889433, 0.077335596, -0.0054512876, -0.04913427, 0.059310008, 0.0049788426, -0.071811624, 0.01686265, 0.03314383, 0.10001295, 0.045063976, 0.09536119, -0.0013537364, -0.060763683, 0.029218901, -0.026311547, -0.025003238, -0.05233236, -0.06105442, 0.004306517, -0.049715742, 0.038377065, 0.007450093, 0.06047295, -0.0096669495, -0.074428245, -0.0058147064, 0.09012795, -0.017298752, -0.04622692, 0.06599692, 0.08663913, -0.05029721, -0.0534953, -0.004869817, -0.04186589, -0.014246032, 0.054076772, 0.00037477602, 0.07326531, -0.008394983, 0.020351473, 0.041575152, -0.05291383, -0.045936182, 0.034742873, 0.045354713, -0.04186589, 0.024567135, 0.017008018, -0.012065516, -0.0045790817, -0.040993683, 0.041284416, 0.077335596, 0.0170807, -0.0022077714, 0.037214123, -0.04041221, -0.0137372445, -0.08896501, -0.08954648, -0.020206105, -0.0633803, -0.14536767, -0.06890427, 0.102338836, -0.030672578, 0.003561508, 0.022241253, 0.008249615, 0.00066778273, 0.074428245, -0.07849854, -0.03983074, 0.015481656, 0.02543934, 0.027038386, 0.06890427, -0.011992833, -0.08256883, 0.024131032, -0.02543934, 0.05029721, 0.002162344, -0.057565596, -0.09652413, 0.10001295, -0.12152737, 0.048262063, 0.04244736, 0.037068754, 0.091872364, 0.047389857, 0.05669339, 0.041284416, 0.020206105, -0.048262063, -0.018389009, 0.032853093, -0.063961774, -0.04651765, 0.005596655, 0.014318715, -0.14246032, -0.063089564, -0.019333899, -0.04797133, -0.03430677, -0.0031980886, 0.057565596, -0.045936182, 0.020206105, 0.008104247, 0.02732912, 0.04477324, 0.047389857, 0.036341917, 0.077335596, 0.071520895, -0.04215662, 0.027183754, -0.030817945, 0.05901927, -0.021659782, -0.029800372, -0.06163589, -0.038667798, 0.054948978, -0.020351473, -0.052041624, 0.02805596, -0.043319564, 0.0971056, -0.023985665, 0.043319564, 0.0096669495, 0.030672578, 0.015627025, -0.04244736, -0.051460154, 0.030091107, 0.036778018, -0.040121477, 0.035324343, 0.0068686223, -0.004324688]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-google-genai Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sIVX2ghbdhpP",
        "outputId": "550b993e-7065-4696-f57f-0e8a1a1665fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-google-genai\n",
            "  Downloading langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Collecting Pillow\n",
            "  Downloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-google-genai)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
            "  Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (0.3.66)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-genai) (2.11.7)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.11/dist-packages (from google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (4.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.32.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.73.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (1.71.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<0.7.0,>=0.6.18->langchain-google-genai) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core<0.4.0,>=0.3.66->langchain-google-genai) (1.3.1)\n",
            "Downloading langchain_google_genai-2.1.6-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.4/47.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-11.3.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading google_ai_generativelanguage-0.6.18-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: filetype, Pillow, google-ai-generativelanguage, langchain-google-genai\n",
            "  Attempting uninstall: Pillow\n",
            "    Found existing installation: pillow 11.2.1\n",
            "    Uninstalling pillow-11.2.1:\n",
            "      Successfully uninstalled pillow-11.2.1\n",
            "  Attempting uninstall: google-ai-generativelanguage\n",
            "    Found existing installation: google-ai-generativelanguage 0.6.15\n",
            "    Uninstalling google-ai-generativelanguage-0.6.15:\n",
            "      Successfully uninstalled google-ai-generativelanguage-0.6.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Pillow-11.3.0 filetype-1.2.0 google-ai-generativelanguage-0.6.18 langchain-google-genai-2.1.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "82adb00129754b869d1ed2c349fac656"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "import base64\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def invoke_rag_model(prompt, matched_items):\n",
        "    \"\"\"\n",
        "    Invokes the Google Gemini multimodal model for a RAG task.\n",
        "    \"\"\"\n",
        "    # Securely get your Google API key if it's not already set\n",
        "    if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "        os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google API Key: \")\n",
        "\n",
        "    # 1. Initialize the ChatGoogleGenerativeAI client with a multimodal model\n",
        "    llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
        "\n",
        "    # 2. Construct the message content for multimodal input\n",
        "    # The format is a list of dictionaries, one for each part (text or image).\n",
        "    message_content = [\n",
        "        {\"type\": \"text\", \"text\": \"Context: You are a helpful assistant for question answering. The following text and images are relevant information that has been retrieved to help you answer the user's question.\"}\n",
        "    ]\n",
        "\n",
        "    # Add the retrieved context (text and images)\n",
        "    for item in matched_items:\n",
        "        if item.get('type') in ['text', 'table']:\n",
        "            message_content.append({\"type\": \"text\", \"text\": item.get('text', '')})\n",
        "        elif item.get('type') == 'image':\n",
        "            # Gemini can take raw image bytes, so no base64 encoding is needed\n",
        "            message_content.append({\n",
        "                \"type\": \"image_url\",\n",
        "                \"image_url\": {\"url\": f\"data:image/png;base64,{base64.b64encode(item.get('image')).decode('utf-8')}\"}\n",
        "            })\n",
        "\n",
        "    # Add the final user prompt\n",
        "    message_content.append({\"type\": \"text\", \"text\": f\"User Question: {prompt}\"})\n",
        "\n",
        "    # 3. Create a single HumanMessage with the multimodal content\n",
        "    messages = [\n",
        "        HumanMessage(content=message_content)\n",
        "    ]\n",
        "\n",
        "    # 4. Invoke the model\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    return response.content"
      ],
      "metadata": {
        "id": "wLLbcc0AKs4r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# User Query\n",
        "query = \"How is it different from prior art. Describe its novelty, use cases.. in detail in layman terms\"\n",
        "\n",
        "# Generate embeddings for the query\n",
        "query_embedding = generate_multimodal_embeddings(prompt=query,output_embedding_length=embedding_vector_dimension)\n",
        "\n",
        "# Search for the nearest neighbors in the vector database\n",
        "distances, result = index.search(np.array(query_embedding, dtype=np.float32).reshape(1,-1), k=5)"
      ],
      "metadata": {
        "id": "44Y37Ee3K2yL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the result (matched chunks)\n",
        "result.flatten()\n",
        "distances"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9Bp55l1LAku",
        "outputId": "6e1910f6-1550-4fb2-847f-ab4ed1030e7b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9509854 , 0.95326436, 0.9546226 , 0.9989011 , 1.0007919 ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the matched items\n",
        "matched_items = [{k: v for k, v in items[index].items() if k != 'embedding'} for index in result.flatten()]\n",
        "\n",
        "# Generate RAG response with Amazon Nova\n",
        "response = invoke_rag_model(query, matched_items)\n",
        "\n",
        "display.Markdown(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "Xa1AhjGJLFXu",
        "outputId": "9ce646b8-4712-4d40-f7a8-fdb8c956f898"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google API Key: ··········\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The Transformer, unlike previous neural sequence transduction models, is the first to rely *entirely* on self-attention to process input and output sequences.  Prior models typically used recurrent neural networks (RNNs) or convolutions, which process data sequentially (one element after another).  This sequential processing limits the ability to parallelize computations, making training slower.\n\n**Novelty:**\n\nThe Transformer's core novelty is its use of **self-attention**.  Imagine you're summarizing a sentence.  Instead of reading the words one by one, self-attention lets the model consider *all* words simultaneously to understand their relationships and importance.  It calculates how much each word \"pays attention\" to every other word in the sentence, determining the context and meaning of each. This allows the model to capture long-range dependencies between words much more effectively than RNNs, which struggle with very long sentences.  This is a key advantage.\n\nAnother novelty is the model's architecture.  It's an encoder-decoder structure, like many before it, but the *implementation* of the encoder and decoder is entirely based on self-attention, discarding the sequential processing of RNNs or convolutions.  The paper also introduces multi-head attention (looking at relationships from different perspectives) and a parameter-free positional encoding (telling the model the order of words without needing extra parameters).\n\n**Use Cases:**\n\nBecause of its speed and ability to handle long sequences, the Transformer architecture has revolutionized many natural language processing tasks.  Examples include:\n\n* **Machine Translation:**  Translating text from one language to another more accurately and efficiently.\n* **Text Summarization:**  Generating concise summaries of longer texts.\n* **Question Answering:**  Answering questions based on given text passages.\n* **Text Generation:**  Creating coherent and contextually relevant text, such as writing stories or articles.\n* **Chatbots:** Building more natural and engaging conversational AI systems.\n\n\nIn layman's terms, the Transformer is like having a super-powered brain for understanding language.  Instead of reading a sentence word-by-word, it looks at all the words at once, instantly grasping the relationships between them. This makes it much faster and better at understanding complex sentences and generating human-quality text.  This has led to significant improvements in many applications that involve processing and generating human language."
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}